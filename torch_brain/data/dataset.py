import os
import logging
import copy
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import copy
import numpy as np


import numpy as np
import h5py
import torch
from temporaldata import Data, Interval


@dataclass
class DatasetIndex:
    """Accessing the dataset is done by specifying a session id and a time interval."""

    session_id: str
    start: float
    end: float


class Dataset(torch.utils.data.Dataset):
    r"""This class abstracts a collection of lazily-loaded Data objects. Each of these
    Data objects corresponds to a session and lives on the disk until it is requested.
    The `include` argument guides which sessions are included in this Dataset.
    To request a piece of a included session's data, you can use the `get` method,
    or index the Dataset with a `DatasetIndex` object (see `__getitem__`).

    This definition is a deviation from the standard PyTorch Dataset definition, which
    generally presents the dataset directly as samples. In this case, the Dataset
    by itself does not provide you with samples, but rather the means to flexibly work
    and accesss complete sessions.
    Within this framework, it is the job of the sampler to provide the
    DatasetIndex indices to slice the dataset into samples (see `kirby.data.sampler`).

    Files will be opened, and only closed when the Dataset object is deleted.

    Args:
        root: The root directory of the dataset.
        split: The split of the dataset. This is used to determine the sampling intervals
            for each session.
        include: A list of dictionaries specifying the datasets to include. Each dictionary
            should have the following keys:
            - brainset: The brainset to include.
            - selection: A dictionary specifying the selection criteria for the dataset.
        transform: A transform to apply to the data. This transform should be a callable
            that takes a Data object and returns a Data object.
    """

    _check_for_data_leakage_flag: bool = True
    _open_files: Optional[Dict[str, h5py.File]] = None
    _data_objects: Optional[Dict[str, Data]] = None

    def __init__(
        self,
        root: str,
        split: str,
        include: List[Dict[str, Any]],
        transform=None,
    ):
        super().__init__()
        self.root = root
        self.split = split

        if include is None:
            raise ValueError("Please specify the datasets to include")

        self.include = include
        self.transform = transform

        self.session_dict = self._look_for_files()

        self._open_files = {
            session_id: h5py.File(session_info["filename"], "r")
            for session_id, session_info in self.session_dict.items()
        }

        self._data_objects = {
            session_id: Data.from_hdf5(f, lazy=True)
            for session_id, f in self._open_files.items()
        }

    def _close_open_files(self):
        """Closes the open files and deletes open data objects.
        This is useful when you are done with the dataset.
        """
        if self._open_files is not None:
            for f in self._open_files.values():
                f.close()
            self._open_files = None

        self._data_objects = None

    def __del__(self):
        self._close_open_files()

    def _look_for_files(self) -> Dict[str, Dict]:
        session_dict = {}

        for i, selection_list in enumerate(self.include):
            selection = selection_list["selection"]

            # parse selection
            if len(selection) == 0:
                raise ValueError(
                    f"Selection {i} is empty. Please at least specify a brainset."
                )

            for subselection in selection:
                if subselection.get("brainset", "") == "":
                    raise ValueError(f"Please specify a brainset to include.")

                # Get a list of all the potentially chunks in this dataset.
                brainset_dir = Path(self.root) / subselection["brainset"]
                files = list(brainset_dir.glob("*.h5"))
                session_ids = sorted([f.stem for f in files])

                if len(session_ids) == 0:
                    raise ValueError(
                        f"No files found in {brainset_dir}. Please check the path."
                    )

                # Perform selection. Right now, we are limiting ourselves to session,
                # and subject filters, but we could make selection more flexible in the
                # future.
                sel_session = subselection.get("session", None)
                sel_sessions = subselection.get("sessions", None)
                # exclude_sessions allows you to exclude some sessions from the selection.
                # example use: you want to train on the complete brainset, but leave out
                # a few sessions for evaluating transfer performance.
                sel_exclude_sessions = subselection.get("exclude_sessions", None)
                sel_subject = subselection.get("subject", None)
                sel_subjects = subselection.get("subjects", None)

                # if subject is needed, we need to load all the files and extract
                # the subject id
                if sel_subject is not None or sel_subjects is not None:
                    all_session_subjects = []
                    for session_id in session_ids:
                        with h5py.File(brainset_dir / (session_id + ".h5"), "r") as f:
                            session_data = Data.from_hdf5(f, lazy=True)
                            all_session_subjects.append(session_data.subject.id)

                filtered = False
                if sel_session is not None:
                    assert (
                        sel_session in session_ids
                    ), f"Session {sel_session} not found in brainset {subselection['brainset']}"
                    session_ids = [sel_session]
                    filtered = True

                if sel_sessions is not None:
                    assert (
                        not filtered
                    ), "Cannot specify session AND sessions in selection"

                    # Check that all sortsets are in the brainset.
                    for session in sel_sessions:
                        assert (
                            session in session_ids
                        ), f"Session {session} not found in brainset {subselection['brainset']}"

                    session_ids = sorted(sel_sessions)
                    filtered = True

                if sel_subject is not None:
                    assert (
                        not filtered
                    ), "Cannot specify subject AND session(s) in selection"

                    assert (
                        sel_subject in all_session_subjects
                    ), f"Could not find subject {sel_subject} in brainset {subselection['brainset']}"

                    session_ids = [
                        session
                        for i, session in enumerate(session_ids)
                        if all_session_subjects[i] == sel_subject
                    ]
                    filtered = True

                if sel_subjects is not None:
                    assert (
                        not filtered
                    ), "Cannot specify subjects AND subject/session(s) in selection"

                    # Make sure all subjects asked for are in the brainset
                    sel_subjects = set(sel_subjects)
                    assert sel_subjects.issubset(all_session_subjects), (
                        f"Could not find subject(s) {sel_subjects - all_session_subjects} "
                        f" in brainset {subselection['brainset']}"
                    )

                    session_ids = [
                        session
                        for i, session in enumerate(session_ids)
                        if all_session_subjects[i] in sel_subjects
                    ]
                    filtered = True

                # Exclude sortsets if asked.
                if sel_exclude_sessions is not None:
                    session_ids = [
                        session
                        for session in session_ids
                        if session not in sel_exclude_sessions
                    ]

                assert (
                    len(session_ids) > 0
                ), f"No sessions left after filtering for selection {subselection['brainset']}"

                # Now we get the session-level information
                config = selection_list.get("config", {})

                for session_id in session_ids:
                    full_session_id = subselection["brainset"] + "/" + session_id

                    if session_id in session_dict:
                        raise ValueError(
                            f"Session {full_session_id} is already included in the dataset."
                            "Please verify that it is only selected once."
                        )

                    session_dict[full_session_id] = dict(
                        filename=(Path(self.root) / (full_session_id + ".h5")),
                        config=config,
                    )

        return session_dict

    def get(self, session_id: str, start: float, end: float):
        r"""This is the main method to extract a slice from a session. It returns a
        Data object that contains all data for session :obj:`session_id` between
        times :obj:`start` and :obj:`end`.

        Args:
            session_id: The session id of the slice. Note this is the fully qualified
                session-id: <brainset>/<session_id>
            start: The start time of the slice.
            end: The end time of the slice.
        """
        data = copy.copy(self._data_objects[session_id])
        # TODO: add more tests to make sure that slice does not modify the original data object
        # note there should be no issues as long as the self._data_objects stay lazy
        sample = data.slice(start, end)

        sample.units.id = np.core.defchararray.add(
            f"{sample.subject.id}/", sample.units.id.astype(str)
            # f"{sample.brainset}/{sample.session}/", sample.units.id.astype(str)
        )

        if self._check_for_data_leakage_flag:
            sample._check_for_data_leakage(self.split)

        sample.session = session_id
        sample.config = self.session_dict[session_id]["config"]
        return sample

    def get_session_data(self, session_id: str):
        r"""Returns the data object corresponding to the session :obj:`session_id`.
        If the split is not "full", the data object is sliced to the allowed sampling
        intervals for the split, to avoid any data leakage. :obj:`RegularTimeSeries`
        objects are converted to :obj:`IrregularTimeSeries` objects, since they are
        most likely no longer contiguous.

        .. warning::
            This method might load the full data object in memory, avoid multiple calls
            to this method if possible.
        """
        data = copy.copy(self._data_objects[session_id])

        # get allowed sampling intervals
        if self.split is not None:
            sampling_intervals = self.get_sampling_intervals()[session_id]
            sampling_intervals = Interval.from_list(sampling_intervals)
            data = data.select_by_interval(sampling_intervals)
            if self._check_for_data_leakage_flag:
                data._check_for_data_leakage(self.split)
        else:
            data = copy.deepcopy(data)

        data.units.id = np.core.defchararray.add(
            # f"{data.brainset}/{data.session}/", data.units.id.astype(str)
            f"{data.subject.id}/", data.units.id.astype(str)
        )
        return data

    def get_sampling_intervals(self):
        r"""Returns a dictionary of interval-list for each session.
        Each interval-list is a list of tuples (start, end) for each interval. This
        represents the intervals that can be sampled from each session.

        Note that these intervals will change depending on the split.
        """
        interval_dict = {}
        for session_id in self.session_dict.keys():
            intervals = getattr(self._data_objects[session_id], f"{self.split}_domain")
            sampling_intervals_modifier_code = self.session_dict[session_id][
                "config"
            ].get("sampling_intervals_modifier", None)
            if sampling_intervals_modifier_code is None:
                interval_dict[session_id] = list(zip(intervals.start, intervals.end))
            else:
                local_vars = {
                    "data": copy.deepcopy(self._data_objects[session_id]),
                    "sampling_intervals": intervals,
                    "split": self.split,
                }
                try:
                    exec(sampling_intervals_modifier_code, {}, local_vars)
                except NameError as e:
                    error_message = (
                        f"{e}. Variables that are passed to the sampling_intervals_modifier "
                        f"are: {list(local_vars.keys())}"
                    )
                    raise NameError(error_message) from e
                except Exception as e:
                    error_message = (
                        f"Error while executing sampling_intervals_modifier defined in "
                        f"the config file for session {session_id}: {e}"
                    )
                    raise type(e)(error_message) from e

                sampling_intervals = local_vars.get("sampling_intervals")
                interval_dict[session_id] = list(
                    zip(sampling_intervals.start, sampling_intervals.end)
                )
        return interval_dict

    def get_session_config_dict(self):
        r"""Returns configs for each session in the dataset as a dictionary."""
        ans = {}
        for session_id in self.session_dict.keys():
            ans[session_id] = self.session_dict[session_id]["config"]
        return ans

    def get_session_ids(self):
        r"""Returns the session ids of the dataset."""
        return sorted(list(self.session_dict.keys()))

    def get_unit_ids(self):
        r"""Returns the unit ids of the dataset."""
        unit_ids = []
        for session_id in self._data_objects.keys():
            data = copy.copy(self._data_objects[session_id])

            supported_formats = ["brainset/session/unit", "brainset/device/unit"]
            unit_ids_format = self.session_dict[session_id]["config"].get(
                "unit_ids_format", "brainset/session/unit"
            )
            if unit_ids_format == "brainset/session/unit":
                unit_ids.extend(
                    [
                        f"{data.subject.id}/{unit_id}"
                        # f"{data.brainset.id}/{data.session.id}/{unit_id}"
                        for unit_id in data.units.id
                    ]
                )
            elif unit_ids_format == "brainset/device/unit":
                unit_ids.extend(
                    [
                        f"{data.brainset.id}/{data.device.id}/{unit_id}"
                        for unit_id in data.units.id
                    ]
                )
            else:
                raise ValueError(
                    f"unit_ids_format {unit_ids_format} is not supported. Supported formats are: {supported_formats}"
                )

        unit_ids = sorted(list(set(unit_ids)))
        return unit_ids

    def get_unit_ids(self):
        r"""Returns all unit ids in the dataset."""
        unit_ids_list = []
        for session_id in self.session_dict.keys():
            data = self._data_objects[session_id]
            unit_ids = data.units.id
            unit_ids = np.core.defchararray.add(
                # f"{data.brainset}/{data.session}/",
                f"{data.subject.id}/",
                unit_ids.astype(str),
            )
            unit_ids_list.extend(unit_ids)
        return unit_ids_list

    def get_session_ids(self):
        r"""Returns all session ids in the dataset."""
        return list(self.session_dict.keys())

    def disable_data_leakage_check(self):
        r"""Disables the data leakage check.

        .. warning::
            Only do this you are absolutely sure that there is no leakage between the
            current split and other splits (eg. the test split).
        """
        self._check_for_data_leakage_flag = False
        logging.warning(
            f"Data leakage check is disabled. Please be absolutely sure that there is "
            f"no leakage between {self.split} and other splits."
        )

    def __getitem__(self, index: DatasetIndex):
        sample = self.get(index.session_id, index.start, index.end)

        # apply transform
        if self.transform is not None:
            sample = self.transform(sample)

        return sample

    def __len__(self):
        raise NotImplementedError("Length of dataset is not defined")

    def __iter__(self):
        raise NotImplementedError("Iteration over dataset is not defined")
