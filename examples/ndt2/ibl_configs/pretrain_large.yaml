defaults:
  - _default
  - dataset: large
  - model: [encoder_large, decoder]
  - sweep
  - _self_

wandb:
  run_name: pretrain_large

num_workers: 8
batch_size: 512

epochs: 1000

optimizer:
  scheduler: true
  lr: 5e-4 # lower it down compared to sweep to avoid unstable training
  start_factor: 0.1
  warmup_steps: 100
  decay_steps: 2500
  lr_min: 1e-6



