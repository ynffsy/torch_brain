{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys \n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import hydra\n",
    "import lightning as L\n",
    "import torch\n",
    "from model import (\n",
    "    BhvrDecoder,\n",
    "    ContextManager,\n",
    "    Decoder,\n",
    "    Encoder,\n",
    "    MaeMaskManager,\n",
    "    SpikesPatchifier,\n",
    "    SslDecoder,\n",
    ")\n",
    "from transforms import FilterUnit, Ndt2Tokenizer\n",
    "from train import DataModule\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from torch import optim\n",
    "from torchmetrics import R2Score\n",
    "from brainsets.taxonomy import decoder_registry\n",
    "\n",
    "from train import TrainWrapper, set_callbacks\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def load_cfg():\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    cfg = OmegaConf.load(\"./configs/train.yaml\")\n",
    "    cfg.data_ssl = OmegaConf.load(\"./configs/data_ssl/odoherty.yaml\")\n",
    "    cfg.data_superv = OmegaConf.load(\"./configs/data_superv/odoherty.yaml\")\n",
    "    del cfg.defaults\n",
    "    return cfg\n",
    "\n",
    "cfg = load_cfg()\n",
    "cfg.wandb.enable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "WARNING:root:Data leakage check is disabled. Please be absolutely sure that there is no leakage between None and other splits.\n",
      "Seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'session': ['odoherty_sabes_nonhuman_2017_v2/indy_20160411_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160411_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20160418_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160419_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160420_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160426_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160622_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160624_03', 'odoherty_sabes_nonhuman_2017_v2/indy_20160630_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160915_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160916_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160921_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20160927_04', 'odoherty_sabes_nonhuman_2017_v2/indy_20160927_06', 'odoherty_sabes_nonhuman_2017_v2/indy_20160930_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20160930_05', 'odoherty_sabes_nonhuman_2017_v2/indy_20161006_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20161007_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20161011_03', 'odoherty_sabes_nonhuman_2017_v2/indy_20161013_03', 'odoherty_sabes_nonhuman_2017_v2/indy_20161014_04', 'odoherty_sabes_nonhuman_2017_v2/indy_20161017_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20161024_03', 'odoherty_sabes_nonhuman_2017_v2/indy_20161025_04', 'odoherty_sabes_nonhuman_2017_v2/indy_20161027_03', 'odoherty_sabes_nonhuman_2017_v2/indy_20161206_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20161207_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20161212_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20161220_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20170123_02', 'odoherty_sabes_nonhuman_2017_v2/indy_20170124_01', 'odoherty_sabes_nonhuman_2017_v2/indy_20170127_03'], 'subject': ['odoherty_sabes_nonhuman_2017/indy']}\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(cfg.seed)\n",
    "\n",
    "if cfg.fast_dev_run:\n",
    "    cfg.wandb.enable = False\n",
    "    cfg.num_workers = 0\n",
    "\n",
    "with open_dict(cfg):\n",
    "    # Adjust batch size for multi-gpu\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    cfg.batch_size_per_gpu = cfg.batch_size // num_gpus\n",
    "    cfg.superv_batch_size = cfg.superv_batch_size or cfg.batch_size\n",
    "    cfg.superv_batch_size_per_gpu = cfg.superv_batch_size // num_gpus\n",
    "    log.info(f\"Number of GPUs: {num_gpus}\")\n",
    "    log.info(f\"Batch size per GPU: {cfg.batch_size_per_gpu}\")\n",
    "    log.info(f\"Superv batch size per GPU: {cfg.superv_batch_size_per_gpu}\")\n",
    "\n",
    "dim = cfg.model.dim\n",
    "\n",
    "# Mask manager (for MAE SSL)\n",
    "mae_mask_manager = None\n",
    "if cfg.is_ssl:\n",
    "    mae_mask_manager = MaeMaskManager(cfg.mask_ratio)\n",
    "\n",
    "# context manager\n",
    "ctx_manager = ContextManager(dim)\n",
    "\n",
    "# Spikes patchifier\n",
    "spikes_patchifier = SpikesPatchifier(dim, cfg.patch_size)\n",
    "\n",
    "# Model = Encoder + Decoder\n",
    "encoder = Encoder(\n",
    "    dim=dim,\n",
    "    max_time_patches=cfg.model.max_time_patches,\n",
    "    max_space_patches=cfg.model.max_space_patches,\n",
    "    **cfg.model.encoder,\n",
    ")\n",
    "\n",
    "if cfg.is_ssl:\n",
    "    decoder = SslDecoder(\n",
    "        dim=dim,\n",
    "        max_time_patches=cfg.model.max_time_patches,\n",
    "        max_space_patches=cfg.model.max_space_patches,\n",
    "        patch_size=cfg.patch_size,\n",
    "        **cfg.model.predictor,\n",
    "    )\n",
    "else:\n",
    "    decoder = BhvrDecoder(\n",
    "        dim=dim,\n",
    "        max_time_patches=cfg.model.max_time_patches,\n",
    "        max_space_patches=cfg.model.max_space_patches,\n",
    "        bin_time=cfg.bin_time,\n",
    "        **cfg.model.bhv_decoder,\n",
    "    )\n",
    "\n",
    "# Train wrapper\n",
    "train_wrapper = TrainWrapper(\n",
    "    cfg, mae_mask_manager, ctx_manager, spikes_patchifier, encoder, decoder\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "ctx_tokenizer = ctx_manager.get_ctx_tokenizer()\n",
    "tokenizer = Ndt2Tokenizer(\n",
    "    ctx_time=cfg.ctx_time,\n",
    "    bin_time=cfg.bin_time,\n",
    "    patch_size=cfg.patch_size,\n",
    "    pad_val=cfg.pad_val,\n",
    "    decoder_registry=decoder_registry,\n",
    "    mask_ratio=cfg.mask_ratio,\n",
    "    ctx_tokenizer=ctx_tokenizer,\n",
    "    inc_behavior=not cfg.is_ssl,\n",
    "    inc_mask=cfg.is_ssl,\n",
    ")\n",
    "\n",
    "# set up data module\n",
    "data_module = DataModule(cfg, tokenizer, cfg.is_ssl)\n",
    "data_module.setup()\n",
    "\n",
    "# register context\n",
    "ctx_manager.init_vocab(data_module.get_ctx_vocab(ctx_manager.keys))\n",
    "\n",
    "L.seed_everything(cfg.seed)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = set_callbacks(cfg)\n",
    "\n",
    "# Set up trainer\n",
    "# trainer = L.Trainer(\n",
    "#     logger=wandb_logger,\n",
    "#     default_root_dir=cfg.log_dir,\n",
    "#     check_val_every_n_epoch=cfg.eval_epochs,\n",
    "#     max_epochs=cfg.epochs,\n",
    "#     log_every_n_steps=cfg.log_every_n_steps,\n",
    "#     callbacks=callbacks,\n",
    "#     accelerator=\"gpu\",\n",
    "#     precision=cfg.precision,\n",
    "#     fast_dev_run=cfg.fast_dev_run,\n",
    "#     num_sanity_val_steps=cfg.num_sanity_val_steps,\n",
    "#     strategy=\"ddp_find_unused_parameters_true\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping 8.99999999999946 seconds of data due to short intervals. Remaining: 18637.0 seconds.\n",
      "/nethome/aandre8/torch_brain/venv/lib/python3.9/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/nethome/aandre8/torch_brain/venv/lib/python3.9/site-packages/lightning/pytorch/core/module.py:447: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.2680, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "train_wrapper.training_step(batch[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, a in train_wrapper.bhv_decoder.named_parameters():\n",
    "#     print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in val_loader:\n",
    "#     break\n",
    "\n",
    "# batch_superv = batch[0]['superv']\n",
    "\n",
    "# batch_superv[\"bhvr_vel\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 150, 32, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_superv[\"spike_tokens\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
